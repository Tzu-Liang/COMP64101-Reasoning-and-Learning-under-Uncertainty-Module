{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf52PGSevKXY"
   },
   "source": [
    "# Assignment Brief: Statistical Inference and Gaussian processes\n",
    "\n",
    "## Deadline: November 18, 2025, 14:00 GMT\n",
    "\n",
    "## Number of marks available: 35\n",
    "\n",
    "This coursework is made of two parts. In the first part, you will explore different techniques for approximate inference. In the second part, you will use Bayesian optimisation for hyperparameter learning. \n",
    "\n",
    "### Please READ the whole assignment first, before starting to work on it.\n",
    "\n",
    "### How and what to submit\n",
    "\n",
    "A. A **Jupyter Notebook** with the code in all the cells executed and outputs displayed.\n",
    "\n",
    "B. Name your Notebook **COM64101_Assignment_part1_XXXXXX.ipynb** where XXXXXX is your username such as such as abc18de. Example: `COM64101_Assignment_abc18de.ipynb`\n",
    "\n",
    "C. Upload the Jupyter Notebook in B to Canvas under the submission area before the deadline.\n",
    "\n",
    "D. **NO DATA UPLOAD**: Please do not upload the data files used in this Notebook. We have a copy already.\n",
    "\n",
    "\n",
    "### Assessment Criteria\n",
    "\n",
    "* Being able to correctly apply frequentist, Bayesian, Monte Carlo, importance sampling, and variational inference methods as specified in each task.\n",
    "\n",
    "* Being able to provide clear comparisons between methods (e.g., MLE vs. Bayesian, plain MC vs. IS, VI vs. HMC) using appropriate metrics, plots, and variance/uncertainty evaluations.\n",
    "\n",
    "* Being able to use Gaussian processes as a surrogate model for Bayesian optimisation of the hyperparameters of a machine learning model.\n",
    "\n",
    "* Being able to concisely explain results, justify methodological choices, and discuss observed differences within the given word limits.\n",
    "\n",
    "### Code quality and use of Python libraries\n",
    "When writing your code, you will find out that there are operations that are repeated at least twice. If your code is unreadable, we may not award marks for that section. Make sure to check the following:\n",
    "\n",
    "* Did you include Python functions to solve the question and avoid repeating code?\n",
    "* Did you comment your code to make it readable to others?\n",
    "\n",
    "### Late submissions\n",
    "\n",
    "We follow Department's guidelines about late submissions, i.e., a deduction of 10% of the mark each 24 hours the work is late after the deadline. NO late submission will be marked one week after the deadline. Please read [this link](https://documents.manchester.ac.uk/display.aspx?DocID=29825).\n",
    "\n",
    "### Academic malpractice\n",
    "\n",
    "**Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.** Please carefully read [what constitutes Academic Malpractice](https://documents.manchester.ac.uk/display.aspx?DocID=2870) if not sure. If you still have questions, please ask your Personal tutor or the Lecturers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjSTJn07PvUe"
   },
   "source": [
    "# 1. Statistical Inference (20 marks)\n",
    "\n",
    "Section 1 of this coursework is made of three parts (Part 1.1, Part 1.2, and Part 1.3). Complete the tasks below. Do **not** modify the provided datasets. \n",
    "\n",
    "For Part 1.1, you need to analyze coin-flip data using both frequentist (MLE + Wald CI) and Bayesian (Beta prior + posterior sampling) methods, compare their predictions for future flips, and understand the differences. For Part 1.2, you need to estimate a 2D integral using plain Monte Carlo and importance sampling. For Part 1.3, you need to fit Bayesian logistic regression using both mean-field variational inference and HMC, compare their posterior approximations (via KL divergence and test log-loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bayesian vs. Frequentist Modelling (5 marks)\n",
    "\n",
    "In this exercise, you will explore two different statistical paradigms—**frequentist inference** and **Bayesian inference**—applied to the problem of estimating a coin’s probability of landing heads. The dataset `coin_experiments.csv` contains 1,000 independent coin-flip experiments, each recording the number of **successes** (heads) and the total **trials**.  \n",
    "\n",
    "**Question**\n",
    "\n",
    "**1.1.A.** The frequentist approach treats the probability of success, $p$, as a fixed but unknown quantity. You will:  \n",
    "- Import the data, print its shape, and check basic sanity (e.g., no missing values, successes ≤ trials). **(0.5 pt)**  \n",
    "- Compute the **Maximum Likelihood Estimate (MLE)**, $\\hat{p} = S/N$, where $S$ is the total number of successes and $N$ is the total number of trials. **(1 pt)**\n",
    "-  Estimate the standard error,  \n",
    "  $$\\text{SE} = \\sqrt{\\hat{p}(1-\\hat{p})/N},$$  \n",
    "  and form a **95% Wald confidence interval** using $z = 1.96$. **(0.5 pt)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.1.B.** The Bayesian approach treats $p$ as a **random variable** with prior distribution. Using a **Beta(1, 1)** prior (uniform), you will:  \n",
    "- Compute the posterior parameters,  \n",
    "  $$\\alpha_{\\text{post}} = \\alpha_0 + S, \\quad \\beta_{\\text{post}} = \\beta_0 + (N-S),$$  \n",
    "  and the **MAP estimate**,  \n",
    "  $$p_{\\text{MAP}} = \\frac{\\alpha_{\\text{post}} - 1}{\\alpha_{\\text{post}} + \\beta_{\\text{post}} - 2}.$$ \n",
    "  **(1 pt)**\n",
    "- Draw at least 5,000 samples from the Beta posterior (e.g., `scipy.stats.beta.rvs`). **(0.5 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.1.C.** Finally, compare predictions for the next **20 flips**:  \n",
    "- From the Bayesian model, simulate posterior predictive outcomes by sampling from a **Binomial(20, p)** where $p$ comes from the posterior samples, and show a histogram/density. **(0.5 pt)** \n",
    "- From the frequentist model, plot the **plug-in Binomial pmf** with $p = \\hat{p}_{\\text{MLE}}$ over the same figure.**(0.5 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.1.D.** Write a short explanation comparing the two predictive distributions. Comment on how the Bayesian posterior predictive accounts for uncertainty in $p$, while the frequentist plug-in relies on a single point estimate. **(0.5 pt)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Variance Reduction with Importance Sampling (5 marks)\n",
    "\n",
    "Many integrals that arise in statistics and machine learning cannot be evaluated analytically, especially in higher dimensions. **Monte Carlo (MC) integration** is a general technique to approximate such integrals by drawing random samples from a distribution and averaging function evaluations. Although MC estimators are unbiased, their variance can be large, meaning we may need many samples to reach a desired accuracy.  \n",
    "\n",
    "A way to improve efficiency is through **importance sampling (IS)**. Instead of sampling from a fixed distribution (e.g., Gaussian), we choose a proposal distribution $q(x)$ that better matches the regions where the integrand contributes most. By reweighting samples, we can reduce variance while keeping the estimator unbiased. The effectiveness of IS depends critically on choosing $q(x)$ with heavier tails or shapes aligned with the integrand.  \n",
    "\n",
    "In this exercise, you will evaluate a 2D integral and demonstrate how importance sampling can achieve significant variance reduction compared to plain Monte Carlo:  \n",
    "\n",
    "$$\n",
    "I = \\int_{\\mathbb{R}^2} \\exp(-\\|x\\|_1)\\,\\sin(\\|x\\|_2)\\,dx,\n",
    "$$  \n",
    "\n",
    "with the goal of achieving an **absolute error < 0.01**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the integrand\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def integrand(x):\n",
    "    l1 = np.sum(np.abs(x), axis=-1)   # L1 norm\n",
    "    l2 = np.linalg.norm(x, axis=-1)   # L2 norm\n",
    "    return np.exp(-l1) * np.sin(l2)\n",
    "\n",
    "# grid\n",
    "xx, yy = np.meshgrid(np.linspace(-4,4,400), np.linspace(-4,4,400))\n",
    "pts = np.stack([xx,yy], axis=-1)\n",
    "zz = integrand(pts)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.pcolormesh(xx, yy, zz, cmap=\"RdBu_r\", shading=\"auto\")\n",
    "plt.colorbar(label=\"f(x)\")\n",
    "plt.title(\"Integrand $f(x) = e^{||x||_1} \\\\, \\\\sin(||x||_2)$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.2.A. Plain Monte Carlo (2 pts)**  \n",
    "   - Estimate $I$ by sampling $x \\sim \\mathcal{N}(0, I_2)$ using the plain Monte Carlo estimator **(1 pt)**:  \n",
    "     $$\n",
    "     \\hat{I} = \\frac{1}{N} \\sum_{i=1}^N f(x_i), \\quad x_i \\sim \\mathcal{N}(0, I_2).\n",
    "     $$  \n",
    "   - Report your estimate $\\hat{I}$ and the **empirical standard error**. Verify that the error decreases as $N$ increases. **(1 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.2.B Importance Sampling (1.5 pts)**  \n",
    "   - Design your own proposal distribution $q(x)$ and implement an importance sampling estimator.**(1 pt)** \n",
    "   - Demonstrate a **variance reduction** for an equal number of samples **(0.5 pt)** \n",
    "   - *Hint:* Consider proposals (e.g., Laplace, Student-t or other) to better capture the integrand’s structure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.2.C RMSE Comparison (1 pt)**  \n",
    "   - Produce a **log–log plot** of RMSE vs. sample size $N$, comparing plain Monte Carlo and importance sampling. **(1 pt)**  \n",
    "   - Use $N \\in [10^3, 10^5]$ (e.g., 5–10 log-spaced points). Ensure both curves are clearly labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.2.D. Justification (0.5 pt)**  \n",
    "   - Explain your choice of proposal $q(x)$. Discuss how it aligns with the shape of the integrand and why it reduces variance relative to Gaussian sampling. **(0.5 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Mean-Field Variational Inference for Bayesian Logistic Regression (10 marks, ★ difficult)\n",
    "\n",
    "In this exercise, you will implement **Bayesian logistic regression** and approximate its posterior distribution using **mean-field variational inference (VI)**. You will then compare VI with a **Hamiltonian Monte Carlo (HMC)** benchmark to highlight the trade-offs between computational efficiency and posterior accuracy. The dataset `log_reg_data.csv` contains predictors `x1, x2, …` and a binary label `y`.\n",
    "\n",
    "### Background & Motivation\n",
    "\n",
    "- **Bayesian logistic regression** models uncertainty in the regression weights $w$, allowing you to quantify predictive uncertainty rather than relying only on a point estimate (as in standard logistic regression). The prior is chosen as a Gaussian:  \n",
    "  $$\n",
    "  w \\sim \\mathcal{N}(0, 10I).\n",
    "  $$  \n",
    "\n",
    "- **Variational Inference (VI)** approximates the true posterior $p(w \\mid D)$ by a simpler distribution. Here, we use a **mean-field Gaussian**:  \n",
    "  $$\n",
    "  q_\\phi(w) = \\mathcal{N}(w \\mid \\mu, \\mathrm{diag}(\\sigma^2)),\n",
    "  $$  \n",
    "  where the parameters $\\phi = (\\mu, \\sigma)$ are optimised to make $q_\\phi$ close to the true posterior.  \n",
    "\n",
    "- Optimisation is done via the **Evidence Lower Bound (ELBO)**:  \n",
    "  $$\n",
    "  \\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi}[ \\log p(D \\mid w) ] - \\mathrm{KL}(q_\\phi \\,\\|\\, p(w)),\n",
    "  $$  \n",
    "  which we estimate stochastically using the **reparameterisation trick**:  \n",
    "  $$\n",
    "  w = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I).\n",
    "  $$  \n",
    "\n",
    "- **Hamiltonian Monte Carlo (HMC)** provides a high-fidelity posterior approximation by simulating from the exact Bayesian posterior using gradient information. It is computationally more expensive but is often treated as the “gold standard” for comparison.  \n",
    "\n",
    "By completing this task, you will see how VI provides a fast but approximate solution, while HMC is slower but more accurate. The comparison highlights the **bias–variance trade-off** in approximate inference.\n",
    "**Hint:**  \n",
    "- Use `autograd` for gradients in VI.  \n",
    "- Clip `log_sigma` to avoid extreme variances.  \n",
    "- Use `scipy.special.expit` for the logistic function.  \n",
    "- For HMC, you can rely on **NumPyro** (already available) to avoid implementing HMC from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.3.A Variational Inference (3 pts)**  \n",
    "   - Construct the VI family and implement the **reparameterised ELBO**. Optimise with SGD/ADAM. **(2 pt)**\n",
    "   - Show a training curve of the ELBO to confirm convergence. **(1 pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autograd import grad\n",
    "import autograd.numpy as anp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is a Python library that works like NumPy but adds extra features such as automatic differentiation and faster computations. NumPyro is a library built on JAX that makes it easier to do Bayesian statistics and probabilistic modeling. You should use JAX together with NumPyro, complete the following task:\n",
    "\n",
    "**Question**\n",
    "\n",
    "**1.3.B HMC Benchmark (2 pt)**  \n",
    "   - Run **4 parallel HMC chains** (NumPyro/Stan acceptable) to obtain ≥1,000 effective samples.  \n",
    "   - Report convergence diagnostics ($\\hat{R} \\leq 1.05$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax\n",
    "!pip install jaxlib\n",
    "!pip install numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro.diagnostics import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.3.C Posterior Comparison (3 pts)**  \n",
    "   - Compute $\\mathrm{KL}(q_\\phi \\parallel p(w \\mid D))$ using HMC samples as the reference posterior. **(2 pt)**  \n",
    "   - Compute test-set log-loss under both VI and HMC. Present results in a table or plot. **(1 pt)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**1.3.D Discussion (2 pt)**  \n",
    "   - Discuss where VI diverges from HMC. Comment on underestimation of uncertainty, mode-seeking bias, and the speed vs. accuracy trade-off.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWKwKL2QVO4Z"
   },
   "source": [
    "# 2. Gaussian processes and Bayesian Optimisation (15 marks)\n",
    "\n",
    "This part of the coursework uses Gaussian processes for Bayesian optimisation of the hyperparameters of a Machine Learning model. The dataset you will use in this assignment comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). The task is  to predict electrical energy output from a [combined cycle Power Plant](https://en.wikipedia.org/wiki/Combined_cycle_power_plant). The description of the dataset can be found [here](https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimisation of Elastic net hyperparameters \n",
    "\n",
    "You will use Thompson Sampling (TS) for hyperparemeter learning of an [elastic net model for regression](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net) over the electrical energy output dataset. Before moving on, please read the mathematical description of the Elastic net model [here](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net). The two hyperparameters to optimise will be the $\\alpha>0$ parameter and the $0\\le\\rho\\le1$ parameter. The $\\alpha$ and $\\rho$ hyperparameters are related to the level of $\\ell_1$ and $\\ell_2$ regularisaion done for the linear regression model. The exact relationship between these parameters is explained [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet).\n",
    "\n",
    "The function we want to minimise will be the Root Mean-Squared Error (RMSE) on a validation dataset,\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{RMSE}(\\alpha, \\rho) = \\sqrt{\\text{MSE}(\\mathbf{y}_\\text{val}, f(\\mathbf{x}_{\\text{val}}, \\mathcal{D}_\\text{train}, \\alpha, \\rho))},\n",
    "\\end{align}\n",
    "\n",
    "where $\\text{MSE}(\\mathbf{y}_\\text{val}, f(\\mathbf{x}_{\\text{val}}, \\mathcal{D}_\\text{train}, \\alpha, \\rho))$ is the Mean-Squared error between the validation data and the prediction of the elastic net model, which in turn is a function of the training data, $\\mathcal{D}_\\text{train}$, and the hyperparameters $\\alpha$ and $\\rho$. \n",
    "\n",
    "**IMPORTANT. You can use scikit-learn for implementing the Elastic Net regressor. You can also use any library for the Gaussian process surrogate model. However, you CAN NOT use any package for the Bayesian optimisation loop. Failure to follow these instructions, will lead to a mark of zero for this section of the assignment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first load the dataset and split it into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "doq = \"https://archive.ics.uci.edu/static/public/294/combined+cycle+power+plant.zip\"\n",
    "pat_sav = \"./combined+cycle+power+plant.zip\"\n",
    "urllib.request.urlretrieve(doq, pat_sav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip = zipfile.ZipFile('./combined+cycle+power+plant.zip', 'r')\n",
    "for name in zip.namelist():\n",
    "    zip.extract(name, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "energy_output = pd.read_excel('./CCPP/Folds5x2_pp.xlsx','Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 9568 observations. We will use a subset of $N_m$ for this exercise. From those, we will select $80\\%$ as the training data, $10\\%$ as the validation data, and $10\\%$ as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_m = 9000\n",
    "ndata, ncols = np.shape(energy_output)\n",
    "np.random.seed(22222)                 # Make sure you use the last five digits of your student UCard as your seed\n",
    "index = np.random.permutation(ndata)  # We permute the indexes  \n",
    "data_tot_red = energy_output.iloc[index[0:N_m], :].copy() # Select N_m points\n",
    "Ne = np.int64(np.round(0.8*N_m))    # We compute N, the number of training instances\n",
    "Neval = np.int64(np.round(0.1*N_m)) # We compute Nval, the number of validation instances   \n",
    "Netest = N_m - Ne - Neval              # We compute Ntest, the number of test instances\n",
    "index = np.random.permutation(N_m)  # We permute the indexes  \n",
    "data_training = data_tot_red.iloc[index[0:Ne], :].copy() # Select the training data\n",
    "data_val = data_tot_red.iloc[index[Ne:Ne+Neval], :].copy() # Select the validation data\n",
    "data_test = data_tot_red.iloc[index[Ne+Neval:N_m], :].copy() # Select the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xe_train = np.concatenate((np.ones((Ne,1)), (data_training.iloc[:, 0:4]).values), axis=1) \n",
    "ye_train = np.reshape((data_training.iloc[:, 4]).values, (Ne,1))\n",
    "Xe_val = np.concatenate((np.ones((Neval,1)), (data_val.iloc[:, 0:4]).values), axis=1) \n",
    "ye_val = np.reshape((data_val.iloc[:, 4]).values, (Neval,1))\n",
    "Xe_test = np.concatenate((np.ones((Netest,1)), (data_test.iloc[:, 0:4]).values), axis=1) \n",
    "ye_test = np.reshape((data_test.iloc[:, 4]).values, (Netest,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initial space filling-design (5 marks)\n",
    "\n",
    "We start by collecting a few initial points from the function we want to optimise. Here, you will assume the first initial design has $n_0 = 5$ points. The ouput of this part of your code should be the data observations ``X0train`` and ``y0train``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write the code to get n_0=5 points for the initial design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implement the sequentail-decision loop to find the optimal set of hyperparameters (5 marks)\n",
    "\n",
    "Assume your optimisation budget is equal to $N = 20$ function evaluations and implement your Bayesian optimiser. For each iteration in the optimisation, you need to:\n",
    "\n",
    "1. Compute the posterior distribution of your Gaussian process using all available training data.\n",
    "2. Use Thompson sampling to find the optimal value to explore next.\n",
    "3. Observe your new output at the suggested optimal point.\n",
    "\n",
    "At the end of the optimisation loop, you should return a value of $\\alpha_*$ and $\\rho_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Compute RMSE in the test set and compare the performance of the Bayes Opt approach against an alterntive method for hyperparameter selection (5 marks)\n",
    "\n",
    "Compute the performance of the Elastic Net model over the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an alternative approach for hyperparameter learning of $\\alpha$ and $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and discuss two interesting findings of this experiment. \n",
    "\n",
    "- *Interesting finding 1*. Write a sentence here of no more than **30 words**.\n",
    "- *Interesting finding 2*. Write a sentence here of no more than **30 words**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3d3a33f663b74facb0f2c3a290f2ee31": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_d4351038f2a8427484333623a29afa0a",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n <span style=\"font-weight: bold\"> Progress                 </span> <span style=\"font-weight: bold\"> Draws </span> <span style=\"font-weight: bold\"> Divergences </span> <span style=\"font-weight: bold\"> Step size </span> <span style=\"font-weight: bold\"> Grad evals </span> <span style=\"font-weight: bold\"> Sampling Speed  </span> <span style=\"font-weight: bold\"> Elapsed </span> <span style=\"font-weight: bold\"> Remaining </span> \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━</span>   3000    0             1.32        3            3121.24 draws/s   0:00:00   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━</span>   3000    0             1.51        3            1698.28 draws/s   0:00:01   0:00:00    \n                                                                                                                   \n</pre>\n",
         "text/plain": "                                                                                                                   \n \u001b[1m \u001b[0m\u001b[1mProgress                \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDraws\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDivergences\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mStep size\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGrad evals\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSampling Speed \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mElapsed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mRemaining\u001b[0m\u001b[1m \u001b[0m \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3000    0             1.32        3            3121.24 draws/s   0:00:00   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3000    0             1.51        3            1698.28 draws/s   0:00:01   0:00:00    \n                                                                                                                   \n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "435b6155dce94d5aab02a878b7940944": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_aab91fab7fa44df3aafb34fb90e39452",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n <span style=\"font-weight: bold\"> Progress                 </span> <span style=\"font-weight: bold\"> Draws </span> <span style=\"font-weight: bold\"> Divergences </span> <span style=\"font-weight: bold\"> Step size </span> <span style=\"font-weight: bold\"> Grad evals </span> <span style=\"font-weight: bold\"> Sampling Speed  </span> <span style=\"font-weight: bold\"> Elapsed </span> <span style=\"font-weight: bold\"> Remaining </span> \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━</span>   3000    0             0.71        3            1584.74 draws/s   0:00:01   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━</span>   3000    0             0.75        3            808.27 draws/s    0:00:03   0:00:00    \n                                                                                                                   \n</pre>\n",
         "text/plain": "                                                                                                                   \n \u001b[1m \u001b[0m\u001b[1mProgress                \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDraws\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDivergences\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mStep size\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGrad evals\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSampling Speed \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mElapsed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mRemaining\u001b[0m\u001b[1m \u001b[0m \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3000    0             0.71        3            1584.74 draws/s   0:00:01   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3000    0             0.75        3            808.27 draws/s    0:00:03   0:00:00    \n                                                                                                                   \n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "aab91fab7fa44df3aafb34fb90e39452": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4351038f2a8427484333623a29afa0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
